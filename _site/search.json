[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ethan Warren",
    "section": "",
    "text": "Hi, I’m Ethan, a 4th-year undergrad student at UCLA specializing in Statistics and Data Science, with a minor in Math. My academic journey has been an exhilarating blend of rigorous coursework, hands-on projects, and collaborative research, enabling me to develop a robust analytical skillset, proficiency in statistical software, and a deep understanding of data-driven decision-making processes.\nI’m excited by the recent developments in artificial intelligence and machine learning, and I’m eager to apply these fields in creative ways to solve complex problems. I’m passionate about leveraging data to solve real-world problems, optimize processes, and inform strategy. As I approach the culmination of my undergraduate studies, I’m eager to apply my skills in a dynamic environment that challenges me to grow and contribute to impactful projects. Here’s a link to my Resume.\nHead to my projects tab to learn more about some of the research and projects I’ve worked on that combine a lot of the things that fascinate me with AI and ML."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Ethan Warren",
    "section": "",
    "text": "Hi, I’m Ethan, a 4th-year undergrad student at UCLA specializing in Statistics and Data Science, with a minor in Math. My academic journey has been an exhilarating blend of rigorous coursework, hands-on projects, and collaborative research, enabling me to develop a robust analytical skillset, proficiency in statistical software, and a deep understanding of data-driven decision-making processes.\nI’m excited by the recent developments in artificial intelligence and machine learning, and I’m eager to apply these fields in creative ways to solve complex problems. I’m passionate about leveraging data to solve real-world problems, optimize processes, and inform strategy. As I approach the culmination of my undergraduate studies, I’m eager to apply my skills in a dynamic environment that challenges me to grow and contribute to impactful projects. Here’s a link to my Resume.\nHead to my projects tab to learn more about some of the research and projects I’ve worked on that combine a lot of the things that fascinate me with AI and ML."
  },
  {
    "objectID": "projects/Classifying_penguins/index.html",
    "href": "projects/Classifying_penguins/index.html",
    "title": "Penguins Classification",
    "section": "",
    "text": "Gentoo penguins playing in the snow.\n\n\nProject Overview\nAs part of an introductory Python class focused on machine learning, our team was tasked with a comprehensive project to classify penguin species using a provided dataset. The dataset contained various attributes of penguins, including but not limited to species type, body measurements (culmen length and depth, flipper length, body mass), and ecological data (island, region, sex, and isotopic composition). Our objective was to conduct a full analysis of this dataset, which entailed extensive data cleaning, handling missing values, and exploratory data analysis to identify significant features for model building.\nThe project’s phases were methodically structured: initially, we performed a thorough analysis to understand the data’s characteristics and distributions. This step was crucial for our subsequent data cleaning process, where we addressed missing entries and potential outliers to ensure the integrity of our modeling data. We then split the data into training and test sets to evaluate the performance and generalizability of our models.\nIn the modeling phase, we explored several algorithms including Multinomial Logistic Regression (MLR), Random Forest, and Support Vector Machines (SVM) to determine which model best fit our data. After a comparative analysis of each model’s performance, Multinomial Logistic Regression was selected as the most effective model based on its accuracy and the interpretability of its results. This model allowed us to classify the penguin species with a high degree of confidence, using the most predictive features derived from our exploratory analysis.\nThis project not only reinforced our understanding of machine learning concepts and techniques but also provided practical experience in data handling, feature selection, and model evaluation—essential skills for any aspiring data scientist.\nData Used\nThe project utilized the Palmer Penguins dataset, a compilation of 344 observations and 17 features including species classification, physical measurements (like culmen length and depth, flipper length, body mass), and ecological data (region, island, sex, dietary isotopes). However, to streamline our analysis and enhance the model’s performance, we refined the dataset by removing columns with excessive missing values or those less relevant for species classification such as comments, sample number, and date of egg collection. The cleansing process was carefully designed to retain potentially predictive attributes, ensuring a robust dataset for building our classification model.\nTools and Technologies\nOur toolset for this project was selected to handle both data manipulation and machine learning tasks effectively:\n\nPython served as the programming language, providing a flexible and powerful base for data science operations.\nSci-Kit Learn was employed for implementing and evaluating various machine learning models due to its extensive library of algorithms and preprocessing methods.\nNumPy and Pandas were crucial for data manipulation tasks, allowing for efficient handling and transformation of data structures.\nMatplotlib and Seaborn were used for exploratory data analysis, enabling us to visualize distributions and relationships in the data effectively, which informed subsequent feature selection and model tuning.\n\nMethodology\nThe methodology adopted for this project was meticulous and divided into several clear stages:\n1. Data Cleaning and Preprocessing\n\nOur team developed a function to clean and preprocess the dataset by dropping non-essential columns, encoding categorical variables, and since there weren’t many, dropping rows with any missing values. This process resulted in a concise dataset with 11 meaningful features, retaining only the rows that provided complete information.\n\n2. Data Splitting\n\nThe data was split into an 80% training set and a 20% test set, and then each set was cleaned and preprocessed independently, ensuring that both sets reflected the full variety of data to avoid model bias.\n\n3. Exploratory Data Analysis (EDA)\n\nPrior to modeling, we conducted thorough exploratory analysis to understand the underlying patterns and relationships in the data. This analysis was pivotal in selecting the most informative features for the models.\n\n\n\nPenguins dataset quantitative features pairplot\n\n\nThe EDA began with grouping the dataset by species and sex to compute summary statistics for each group across all quantitative features. To visualize the relationships between variables, we utilized Seaborn’s pair plot tool, color-coding each of the three penguin species. This visualization helped us identify significant clusters, indicating strong differentiation potential between species based on certain feature combinations.\n\n\n\nHistograms for Culmen (penguin beak) length and depth, grouped by species, to highlight group differences and a potential way to identify species given these measurements.\n\n\nNotably, the relationships between culmen length vs. flipper length, body mass vs. culmen length, and culmen depth vs. culmen length exhibited distinct clusters that were promising for classification. Additionally, histograms for culmen length and culmen depth—grouped by species—revealed unique patterns. These distributions showed some overlap when considered individually but, when combined, suggested a high potential for accurately classifying species. We also examined these features split by sex, revealing further distinctions in measurements that could enhance our model’s predictive accuracy. The combination of culmen length and depth, in particular, emerged as critical: distinguishing between Adelie and the combined group of Chinstrap/Gentoo by culmen length, and between Gentoo and the combined group of Adelie/Chinstrap by culmen depth. This structured approach not only facilitated a thorough understanding of the dataset but also ensured that our machine learning models were built on a solid foundation of clean and relevant data.\n4. Feature Selection\n\nInformed by our EDA, we moved to feature selection, aiming to identify a compact set of features that could effectively predict penguin species. Given the patterns observed, we chose to focus on two quantitative features—culmen length and depth—and one qualitative feature—sex—as our primary predictors.\nTo refine our selection and validate the effectiveness of these features, we devised a function named comb_score. This function systematically tested combinations of these features across our three chosen models: Multinomial Logistic Regression, Random Forests, and Support Vector Machines. Employing cross-validation as part of our strategy allowed us to mitigate overfitting and ensure robustness in our model’s performance. Cross-validation was particularly vital, as it provided a more generalized understanding of model accuracy across multiple subsets of the data, rather than relying solely on a single train-test split.\nThis approach not only streamlined our feature set but also reinforced our model’s capacity to generalize well to new data, an essential aspect of successful machine learning applications. By the guidelines of the project, we were to select 2 quantitative variables and 1 qualitative variable. From the results of running this function, we found that the three most appropriate variables to include in the model were ‘Culmen Length’, ‘Culmen Depth’, and ‘Sex’.\n5. Modeling\nTo analyze the data, we employed three distinct machine learning models using the Sci-kit Learn library: Multinomial Logistic Regression (MLR), Random Forest, and Support Vector Machines (SVM). Each model was evaluated for its effectiveness in classifying the species of penguins based on the selected features.\n\nMultinomial Logistic Regression (MLR): We began with the MLR model due to its robustness in handling multiclass classification problems. After training the model, we utilized a confusion matrix to assess its performance. The confusion matrix demonstrated perfect classification accuracy on the test set, with a cross-validation score of 0.988. Furthermore, we plotted decision regions to visualize the classification boundaries created by the model. These plots revealed that most data points were correctly classified, with only a few outliers, emphasizing the model’s overall high accuracy and reliability.\n\n\n\n\nDecision regions as predicted by multinomial logistic regression model, colored by species.\n\n\n\nRandom Forest: Our next model was the Random Forest classifier. After training, similar evaluations were conducted. The confusion matrix indicated that the Random Forest model successfully predicted the species of 63 out of 64 penguins in the test set, only misclassifying a single Gentoo penguin as a Chinstrap. The cross-validation score for this model was 0.981. Decision region plots were again used to visualize the model’s performance, showing a high degree of accuracy, with minimal misclassification except for the noted outlier.\n\n\n\n\nDecision regions as predicted by randon forest model, colored by species.\n\n\n\nSupport Vector Machines (SVM): Finally, we trained the SVM model. This model yielded a cross-validation score of 0.827, significantly lower than the other two models. The confusion matrix revealed several misclassifications, particularly misidentifying 7 Gentoo penguins as Chinstrap. This issue was further explored through decision region plots, which highlighted a large overlap in the prediction regions, especially where the SVM used culmen depth as a key feature. It became evident that the model struggled with overlap in feature distributions, particularly for Gentoo penguins with culmen depths less than approximately 18-19 mm, leading to these misclassifications. Each model’s decision regions provided critical insights into how effectively each algorithm could delineate between species based on the given features. This comparison highlighted strengths and weaknesses in how each model handled the complexity of the dataset, guiding our ultimate choice of the best performing model for the final application.\n\n\n\n\n\nDecision regions as predicted by support vector machines model, colored by species.\n\n\nResults\n\nMultinomial Logistic Regression (MLR) The Multinomial Logistic Regression model achieved a cross-validation score of 0.988, indicating high accuracy in predicting penguin species based on culmen length, culmen depth, and sex. The confusion matrix confirmed that the model correctly classified all penguins in the test dataset. Despite a few points misclassified as outliers, the decision regions plot demonstrated that the model performs robustly across the dataset. This suggests that the MLR model is not overfitting, as it maintains consistent performance across training, cross-validation, and testing datasets, indicating good generalization to new data. Possible improvements could include expanding the dataset and incorporating more predictive features to enhance the model’s accuracy further.\nRandom Forest The Random Forest model reported a cross-validation score of 0.981, showcasing its effectiveness in species classification using the same features as MLR. It misclassified only one penguin in the test set, suggesting it is almost as accurate as the MLR model. The decision regions plot indicated no signs of overfitting, with stable performance across different data splits. This model’s generalizability could potentially be improved through hyperparameter tuning, such as GridSearch or Bayesian optimization, to optimize performance.\nSupport Vector Machines (SVM) The Support Vector Machines model lagged behind the other two, with a cross-validation score of 0.827. It incorrectly predicted the species of 7 out of 64 penguins in the test set, primarily confusing Gentoo penguins as Chinstrap. This issue was visible in the decision regions plot, where the model showed significant overlap in species classification. Despite its lower performance, the SVM model did not exhibit signs of overfitting and performed consistently across the training and testing datasets. Improvement could be achieved by expanding the number of qualitative variables used for prediction, which preliminary tests suggested might enhance accuracy.\n\nOverall Performance and Recommendations:\nThe MLR and Random Forest models both achieved near-perfect prediction accuracy on the test data, making either model a strong candidate for practical applications. However, given its slightly higher cross-validation score and simpler structure, the MLR model is recommended as the primary choice. Its simplicity suggests better generalizability and lower computational overhead compared to the more complex Random Forest model.\nIn conclusion, while all three models performed commendably, the Multinomial Logistic Regression model, using culmen length, culmen depth, and sex as predictors, is selected as the optimal model for classifying penguin species due to its high accuracy and model simplicity.\nChallenges\nThroughout the project, we faced several challenges that tested our problem-solving and analytical skills. One of the primary challenges was working with these models in python, as they were all brand new to us as students. We conducted a lot of research and consulted the documentation from sci-kit learn heavily. Another challenge was determining the best methods for feature selection and model evaluation, especially in balancing model accuracy with the risk of overfitting. To tackle this, we were able to plot the decision regions of the model, which also gave us a lot of helpful insight into how the models were making their classifications, and where the cutoff for the features was. These challenges necessitated a thorough exploration of various statistical techniques and collaborative problem-solving to ensure the integrity and reliability of our findings.\nTeamwork\nThe success of this project was largely due to effective teamwork and collaborative effort. All team members were actively involved in every phase of the project, from data acquisition and preparation to model evaluation and final discussions. In the initial stages, we jointly worked on cleaning the data, which involved rigorous discussions on handling missing values and deciding which features to modify or remove. During the data visualization phase, each of us contributed by coding at least one plot or table, ensuring diverse input in the discussion and interpretation of results.\nFeature selection was a collective effort where we debated various approaches and jointly developed the cross-validation scoring function, primarily coded by Chen and Lucas. Model fitting and analysis were also collaborative, with Ethan (myself) taking the lead on coding the decision regions plots. We collectively analyzed the outputs, including confusion matrices and the implications of misclassifications by each model. Our final decision on the model selection was made unanimously, reflecting our cohesive approach to research and problem-solving.\nConclusion\nThis project not only enhanced our technical skills in data science and machine learning but also strengthened our ability to work as a team. Throughout the project, we successfully navigated challenges through mutual support and shared knowledge, which was crucial in achieving our objectives. The experience has prepared us well for future collaborative projects and professional challenges in the field of data science. Read my full report of the project here. And just for fun for reading this far, here is a super cute video of Gentoo Penguins building nests."
  },
  {
    "objectID": "projects/Sentiment_tweets/index.html",
    "href": "projects/Sentiment_tweets/index.html",
    "title": "Mental Gymnastics",
    "section": "",
    "text": "Simone Biles completing a beam routine at the Tokyo 2020 Olympics, days before dropping out due to mental health concerns."
  },
  {
    "objectID": "projects/Sentiment_tweets/index.html#introduction",
    "href": "projects/Sentiment_tweets/index.html#introduction",
    "title": "Mental Gymnastics",
    "section": "Introduction",
    "text": "Introduction\nIn recent years, the intersection of mental health and athletic performance has garnered significant attention, particularly within the context of high-profile athletes. Recent discussions are challenging existing stigmas and promoting a new culture of openness and support. This study focuses on analyzing public sentiment towards the mental health of athletes by examining tweets posted about Simone Biles after dropping out of the Olympic Competition in Tokyo, following mental health concerns.\nSimone Biles’ public discourse provides a rich playground for exploring societal attitudes towards mental health in the sports domain, using twitter as proxy for public response. Utilizing natural language processing techniques (NLP), this report examines the nuances of public opinion, shedding light on prevalent themes and sentiments expressed on social media. Understanding these perspectives is crucial for addressing stigma and promoting mental health awareness among athletes. This study aims to contribute to the broader conversation on mental health in sports, offering insights into the public’s response to athletes’ mental health challenges. Click here to find the gihub for the project where all the code and documentation can be found.\n\nProject Overview\nThe primary objective is to understand how public discourse, as seen in Twitter social media, frames mental health issues within the realm of professional international sports. We use sentiment analysis to capture the attitude of the public’s reaction. We then use topic modelling to identify common themes present in the tweets, and finally word vectorization to map topics to words and emotions."
  },
  {
    "objectID": "projects/Sentiment_tweets/index.html#methodology",
    "href": "projects/Sentiment_tweets/index.html#methodology",
    "title": "Mental Gymnastics",
    "section": "Methodology",
    "text": "Methodology\n\nData Collection\nThe dataset was compiled from public tweets using the Twitter API, isolating tweets mentioning gymnastics during the 2020 Summer Olympics. The collection period spanned from July 15 to August 14, 2021, capturing the timeline of Simone Biles’s participation and subsequent withdrawal. The dataset contains around 16.5K tweets. The features present in this dataset include: the textual content of each tweet, any entities present (hashtags, mentions, urls, etc.), the time posted, and the unique twitter ID of both the author and the tweet.\nNotably, any information related to engagement metrics, i.e. likes, comments, replies, and retweets, is not part of the dataset. This required us to direct our analytical focus towards natural language processing due to the data’s qualitative nature.\nThe cleaned data consisted of the following fields:\n\n\n\nVariable\n\n\nDescription\n\n\n\n\nText\n\n\nContent of the tweet\n\n\n\n\nEntities\n\n\nVarious entities mentioned in the tweet, such as hashtags and annotations\n\n\n\n\nTime\n\n\nThe time the tweet was posted\n\n\n\n\nOther\n\n\nOther information, e.g. unique tweet ID, author ID, and edit history were not utilized in this analysis\n\n\n\n\n\nExploratory Data Analysis\nWe began our analysis by exploring the dataset to understand the dataset on a broad level before diving into an analysis. We began by creating a word cloud of the tweet textual data.\n\n\n\nFig 1. Word Cloud\n\n\nOlympics, gymnastics, and Simone Biles are all central to this word cloud, indicating a high volume of tweets that contained these words. Other recurrent terms such as “tokyo2020,” “team,” “gold,” and “sport” further characterize the dataset’s focus around gymnastics and the Olympic Competition.\nWe then looked at the number of tweets per hour and how they evolved over time after Simone Biles made the announcement to drop out of the Olympics. The team used an eight-hour rolling mean to smooth out the curve over a longer time period while still retaining the shape of the data.\n\n\n\nFig 2. Tweet volume per 8 hours.\n\n\nWe see a clear spike right after her announcement, indicating a public response on the day she decided to drop out; however, we also see the number of tweets decrease relatively quickly after August 27, returning to what seems to be normal olympic tweet volume after about a week, and then to low tweet activity after just 2 weeks. This suggests there was not a sustained public reaction.\nFinally, we looked at the top 10 most common hashtags present.\n\n\n\nFig 3. Bar Chart of Hashtags.\n\n\nWe can see again an strong focus on gymnastics, with the olympics, and Simone Biles also being important hashtags. We see the first mention of mental health in the 9th most common hashtag, indicating that it is a relevant topic to this dataset, but appears not to be the primary focus of the tweets. With a better understanding of the content of the data, we move on to natural language processing.\n\n\nNatural Language Processing\n\nPreprocessing\nTo prepare for NLP, we further preprocessed the tweet data to standardize it, focusing on the pure textual content. We followed preprocessing protocol by first removing all punctuation and special characters, as well as any stop words like “the,” “is,” and “and” which don’t carry meaningful information. Tweets were then lemmatized, breaking down words to their root form (e.g. ‘flipping’ \\(\\rightarrow\\) ‘flip’), which typically improves NLP results. We did our best to remove junk tweets, and decided to remove all links and url patterns from the text as well, to focus on the textual content of each tweet.\n\n\nSentiment Analysis\nFor sentiment analysis, we employed the Valence Aware Dictionary and sEntiment Reasoner (VADER) tool, which is well-suited for social media text due to its consideration of contextual polarity and intensity. VADER assigns each tweet a sentiment score based on a predefined lexicon. The sentiment score ranges from -1 to 1, where:\n\nNegative sentiment: Scores closer to -1 indicate negative sentiment.\nNeutral sentiment: Scores around 0 indicate neutral sentiment.\nPositive sentiment: Scores closer to 1 indicate positive sentiment.\n\nEach tweet in the dataset was processed through the VADER tool to generate sentiment scores. The distribution of sentiment scores across the dataset was analyzed and the proportion of positive, negative, and neutral tweets, was calculated. Further analysis was conducted to compare sentiment scores from tweets that came from before and after Biles’ announcement, and isolating tweets that specifically discuss mental health to identify different sentiment patterns.\nThe sentiment analysis results set the foundation for further investigation into the public’s perception of athletes’ mental health and provided a contextual basis for the subsequent topic modeling.\n\n\nNon-negative Matrix Factorization\nTo further understand the themes and discussions within tweets about Simone Biles, we employed topic modeling, a technique designed to discover abstract topics within a collection of documents. This study utilized Non-Negative Matrix Factorization (NMF), a method that is particularly effective for high-dimensional text data, providing clearer and more interpretable topics.\nWhile many topic modelling techniques were initially tested, like Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA), NMF was chosen for its robustness in handling large text datasets and its ability to produce easily interpretable topics. NMF works by factorizing the term-document matrix into two lower-dimensional matrices: a term-topic matrix and a topic-document matrix. Each document (tweet) is represented as a mixture of topics, and each topic is represented as a mixture of terms.\nImplementation:\n\nLibrary: The NMF model was implemented using Python’s Scikit-learn library.\nParameter Selection: The number of topics was determined by evaluating the coherence scores and conducting manual inspections to ensure meaningful and distinct topics.\nProcess: The preprocessed tweet data was vectorized using Term Frequency-Inverse Document Frequency (TF-IDF) to create the term-document matrix. This matrix was then decomposed using NMF.\n\nThe resulting topics were analyzed and ordered by thematic prominence within the dataset. Each topic was defined by a set of high-weighted words, providing insights into the key themes discussed in relation to Simone Biles’ mental health.\nTo gain further insights into these topics, the top 10 weighted words from each topic were embedded into a vector space using the word2vec Google News 300 word embeddings. These vectors were averaged to generate a single vector per topic, which was then mapped back to the closest word in the embedding space, identifying a single representative word for each topic. Additionally, the emotional sentiment of each topic was inferred using a similar embedding approach, associating each topic with the closest emotion word."
  },
  {
    "objectID": "projects/Sentiment_tweets/index.html#results",
    "href": "projects/Sentiment_tweets/index.html#results",
    "title": "Mental Gymnastics",
    "section": "Results",
    "text": "Results\n\nSentiment Analysis\nThe sentiment analysis revealed insightful trends in the public’s reaction to Simone Biles’ mental health disclosure. A histogram plot of the sentiment scores distribution for all tweets in the dataset indicated that the majority of sentiment scores were close to neutral, with a slight skew towards positive sentiments. This suggests that, overall, the public sentiment towards Simone Biles during this period was more supportive than critical. This surprised the team, as we expected a significant negative sentiment to be present.\n\n\n\nFig 4. Histogram distribution of sentiment scores for tweets.\n\n\nFurther analysis was conducted to compare sentiment scores before and after Biles’ announcement. Histogram plots for each period visually appeared similar, implying no significant change in sentiment of tweets before and after Biles announced her withdrawal. The average sentiment score before her withdrawal was 0.172, and after was 0.174. A t-test confirmed no statistically significant difference between the means, indicating that Biles’ announcement did not drastically alter public sentiment.\n\ninsert figure *\n\nAnother split on the dataset was conducted to examine whether tweets specifically discussing mental health exhibited different sentiment patterns, which was done by isolating tweets containing mental health awareness keywords and hashtags. The sentiment distribution of these tweets, compared to those without such keywords, showed that mental health-related tweets had slightly more extreme positive sentiment scores. The average sentiment for mental health tweets was 0.173, while for non-mental health tweets, it was 0.163. However, a t-test found no significant difference between these means, suggesting that the overall sentiment towards mental health topics in the context of Biles’ situation remained consistent with the general sentiment.\n\ninsert figure *\n\n\n\nTopic Modelling\nThe topic modeling analysis using Non-Negative Matrix Factorization (NMF) successfully identified several key themes within the tweets. In NMF, topics are ordered by prevalence, and each topic is defined by a set of high-weighted words, as well as its average word and most closely associated emotion. The 5 most prominent topics among this set of tweets are listed below (Fig 6.).\n\n\n\n\nTopic\n\n\nAssociated Words\n\n\nAverage Word\n\n\nClosest Emotion\n\n\n\n\n1\n\n\nolympics, watching, beam\n\n\nwatch\n\n\nlove\n\n\n\n\n2\n\n\nmisogyoir, racism, sexism\n\n\nracism\n\n\nhostility\n\n\n\n\n3\n\n\nsimonebiles, mental, health\n\n\nmental\n\n\nloneliness\n\n\n\n\n4\n\n\nsuni lee, bars, amazing\n\n\ncongrats\n\n\nlove\n\n\n\n\n5\n\n\nrapinoe, zanetti, mamute\n\n\nwin\n\n\nadmiration\n\n\n\n\n\nTopic 1: This topic clearly encapsulates the general enthusiasm and engagement with the Olympic events. The closest word to this topic vector was “watch,” with the associated emotion being “love,” indicating a positive and supportive sentiment among the public towards the Olympics.\nTopic 2: This topic appears to center on discussions of racial and gender-based discrimination. The word “racism” was the closest match to the topic vector, with “hostility” being the closest emotion, reflecting the critical and adversarial nature of these conversations.\nTopic 3: This topic appears to capture the conversation surrounding Biles and her mental health struggle. The closest word representing this topic was “mental,” with “loneliness” as the associated emotion. This suggests a strong emphasis on mental health issues and a sense of isolation or struggle associated with such discussions.\nTopic 4: This topic captured expressions of admiration and congratulations, particularly directed at fellow gymnast Suni Lee. The closest word was “congrats,” with the emotion “love,” indicating a celebratory and supportive sentiment.\nThe most dominant topic here suggests that a significant portion of the twitter discourse on gymnastics was driven by general excitement and positive engagement with the Olympics and Team USA’s performance. This finding indicates that despite the gravity of Biles’ announcement, the overarching public attention remained on the broader spectacle of the Games.\nThe presence of topics like “racism,” and “sexism” highlight that discussions around Biles’ withdrawal also inevitably intersect with broader societal issues, indicating that public discourse on mental health in athletes is not isolated but intertwined with other dimensions of social justice, underscoring the complexity of public perceptions and the multifaceted nature of the discourse.\nThe third topic directly relates to mental health, and indicates that while there was substantial discourse focused on Biles’ mental health, it was less prevalent than the excitement for the Olympics and other discussions of racism and sexism social issues. The sentiment of “loneliness” associated with this topic also seems to suggest that public discussions on mental health were tinged with a recognition of the isolation and personal struggle involved. This could imply empathy for Biles, as well as an acknowledgment of the broader challenges faced by athletes when they address mental health issues publicly, but overall the conversations captured by this theme seem to be sympathetic towards Biles and her mental health struggles."
  },
  {
    "objectID": "projects/Sentiment_tweets/index.html#conclusions",
    "href": "projects/Sentiment_tweets/index.html#conclusions",
    "title": "Mental Gymnastics",
    "section": "Conclusions",
    "text": "Conclusions\nRegarding the initial research statement, which sought to understand the public sentiment towards athletes’ mental health, the findings suggest a dual narrative. On one hand, the significant focus on mental health and the empathetic sentiment of “loneliness” indicate that the public is becoming more aware and potentially more supportive of athletes prioritizing their mental well-being. On the other hand, the lesser prominence of this topic compared to others highlights that while there is growing support, mental health is still not the dominant narrative in public discourse surrounding athletes.\nFurthermore, the intersection of mental health discussions with broader societal issues such as racism and sexism reveals that public sentiment towards athletes’ mental health cannot be viewed in isolation. It is intricately linked with other dimensions of social justice, indicating that the support for mental health is also influenced by the public’s stance on these broader issues.\nIn conclusion, the public sentiment towards athletes’ mental health, as observed through the analysis of tweets during the Tokyo Olympics, appears to be empathetic yet secondary to broader themes of Olympic excitement and social justice discussions. The discourse reflects a growing recognition of the importance of mental health but also underscores the need for continued efforts to elevate these conversations to the forefront of public attention. This duality emphasizes the ongoing challenge of destigmatizing mental health issues in the athletic arena and fostering a more supportive environment for athletes addressing these concerns.\n\nLimitations and Future Research\nWhile this analysis provides valuable insights into public discourse surrounding Simone Biles’ withdrawal from the Tokyo Olympics, several limitations should be acknowledged. One significant limitation is the focus on a single event and specific athlete, which may not capture the full spectrum of public sentiment towards athletes’ mental health. Future research should aim to gather and analyze data related to public discourse on mental health in other high-profile athletes. By examining a broader range of incidents where athletes have publicly addressed their mental health, researchers can achieve a more comprehensive understanding of public sentiment.\nAdditionally, expanding the temporal scope of data collection would be beneficial. By examining events over a longer period, researchers can identify trends and shifts in public discourse, enhancing the understanding of how societal attitudes towards athletes’ mental health evolve and what factors influence these changes. Including data from other social media platforms and news sources could also provide a more holistic view of public sentiment, as different platforms may host varied demographics and types of discourse, offering a richer and more nuanced understanding of public opinion.\nFuture research should also delve deeper into the intersectionality of issues identified in the current study, such as how discussions on mental health intertwine with topics like racism and sexism across different contexts and athlete demographics. Employing more sophisticated sentiment analysis techniques, such as deep learning-based models, could further enhance the accuracy and depth of emotional sentiment extraction from the text, uncovering subtler nuances in the public’s emotional reactions. These advanced methods would contribute to a more detailed and precise understanding of public sentiment towards athletes’ mental health.\n.\n.\n.\n.\n.\n.\n.\n.\n.\nWhile this analysis provides valuable insights into public discourse surrounding Simone Biles’ withdrawal from the Tokyo Olympics, some limitations should be acknowledged.\nOne key limitation is the focus on a single event and a specific athlete, which may not capture the full spectrum of public sentiment towards athletes’ mental health. Future research may aim to gather and analyze data related to public discourse on mental health in other high-profile athletes. Gathering data on a broader range of incidents where athletes have publicly addressed their mental health would provide a more comprehensive understanding of public sentiment.\nAdditionally, more diversity in data through time would be beneficial. By examining events over a longer period of time, researchers can identify trends and shifts in public discourse. This could help in understanding how societal attitudes towards athletes’ mental health evolve and the factors influencing these changes.\nMoreover, expanding the analysis to include other social media platforms and news sources could provide a more holistic view of public sentiment. Different platforms may host varied demographics and types of discourse, offering a richer and more nuanced understanding of public opinion.\nAnother avenue for future research is to delve deeper into the intersectionality of issues identified in the current study. Investigating how discussions on mental health intertwine with topics like racism and sexism across different contexts and athlete demographics could provide further insights into the complexities of public perception.\nLastly, employing more sophisticated sentiment analysis techniques, such as deep learning-based models, could enhance the accuracy and depth of emotional sentiment extraction from the text. These advanced methods may uncover subtler nuances in the public’s emotional reactions."
  },
  {
    "objectID": "projects/Super_mario_RL/index.html",
    "href": "projects/Super_mario_RL/index.html",
    "title": "Super Mario Reinforcement Learning",
    "section": "",
    "text": "Original Super Mario Bros.\n\n\n\nIntroduction\nIn a world where Mario doesn’t need your thumbs to save the princess, a neural network instead takes up the mantle of heroism, bounding over pits and stomping Goombas with algorithmic enthusiasm. This was the vision of this project that I, and three other students—Tony Lei, Bree Chen, and Alyssa Lui—worked on to create and train an unsupervised learning neural network to play Super Mario Bros. Born from a blend of nostalgia and curiosity, our objective was to explore the capabilities of unsupervised learning in a complex, dynamic environment, demonstrating how artificial intelligence can learn and adapt without explicit human supervision.\n\n\nGoals and Overview\nThe primary goal of our project was to develop an AI agent capable of playing Super Mario Bros. autonomously, learning game mechanics and strategies through exploration and experimentation. The algorithm by which our AI learns to play the game is known as an unsupervised reinforcement learning algorithm, which is notable in how it closely mimics the way a human player might learn to play a game: the AI agent starts with no knowledge of how the game works, and through trial and error of—essentially—random button pressing, learns a strategy to play the game.\n\nFramework\nTo dive a little deeper into some of the high level of how this algorithm worked, we used a framework called a dual network Unsupervised learning is a process whereby an AI learns to achieve a specific output essentially through trial and error. The AI is allowed the freedom to explore the world and find gameplay strategies on its own. This type of learning encourages it to rely on pattern recognition and visual observation to guide its actions, showcasing the AI’s potential to adapt and generalize in unpredictable scenarios.\nYet even the most astute observer needs feedback to refine their actions, and herein lies the synergy between unsupervised learning and reinforcement learning. To give our AI a sense of how to improve over time, we defined clear metrics to serve as indicators of success. There are many different indicators you could use here, score being an obvious one, but we decided to reward our AI for just moving closer to the flagpole. This type of reward tends to prioritize beating levels as fast as possible.\nThen, each time the AI made a decision—whether it was to jump, run, or stand still—it received feedback based on the outcome. Successfully moving to the right increased its score, while dying or failing to navigate an obstacle reduced it. Through iterative gameplay, the AI used this feedback to adjust its strategies, gradually honing its performance, much like a human would.\n\n\n\nMethodology\nThis section will go over some of the more technically involved parts of the design and implementation of this project.\n\nImplementation Framework\nThe implementation of this project in code follows this extremely helpful and detailed tutorial on youtube, from Sourish Kundu. This video is amazing, going over the theoretical math behind this project, and showing how to actually implement that theoretical framework in code.\nThe implementation involved setting up the environment with OpenAI’s Gym, designing the neural network architecture, and developing the training loop to iteratively update the Q-values and improve the agent’s performance. We made sure to adapt and expand upon the concepts introduced in the video to tailor the project to our specific goals and requirements.\n\n\nArchitecture\nThe Double Deep Q Network, or DDQN, is a sophisticated variant of the Deep Q Network designed to mitigate overestimation bias in action-value estimation from traditional reinforcement learning. It does this by having two neural networks working together. The online neural network makes decisions for mario, and the target network predicts the future rewards from taking that action. This architecture was chosen for its precision and reliability and its ability to stabilize training and to accelerate convergence of the network.\nThe neural networks are comprised of 3 convolutional layers to process visual data and 3 fully connected layers to drive decision-making, using ReLU activations. The convolutional layers process input frames to detect features like edges, textures, and objects. By applying what are called convolutional filters, the network learns to recognize patterns such as enemies, obstacles, and movements within the game. After the convolutional layers, the fully connected layers combine the extracted features and predict the the best possible action to take in a given game state.\nSimplification Choices\nTo make the training process feasible and efficient, we incorporated several simplification choices:\n\nPreprocess Frames: We preprocess the game frames using wrappers to reduce computational complexity and simplify training. A SkipFrame wrapper reduces the frequency of updates by repeating the same action for a fixed number of frames. Frames are converted to grayscale to reduce the amount of data the network must process. They are resized to a smaller, standardized size (84x84 pixels) to decrease computational load. Consecutive frames are then stacked to provide temporal context, which helps the agent understand movement and changes in the environment.\nLimited Action Space: We restricted the agent’s action space to a simplified set of actions (e.g., moving right, jumping) to focus on the essential movements needed to navigate the level. This reduces the complexity of the decision-making process and speeds up learning, though it does limit flexibility of the model as Mario is only allowed to perform movements that move him towards the flagpole.\nReward Structure: The reward system incentivizes the agent to progress through the level by awarding positive rewards for moving right and penalizing it for standing still or dying. This straightforward reward structure guides the agent towards the main goal of completing the level.\n\n\n\nTraining Process\nTraining our AI involved a reinforcement learning loop, a cycle of action and feedback. At each step, the AI was fed the current frame state, and the online neural network chose an action—jump, run, or stand still—based on its current understanding, and received feedback in the form of rewards or penalties. This feedback guided the AI to adjust the weights of its neural network to take better actions in the next iteration, slowly converging on game play that maximized the feedback objectives.\nTo enhance training stability, we utilized a replay buffer. This technique stores past experiences and refers to them during training, allowing the AI to learn from a diverse set of past scenarios and reduce the risk of overfitting by breaking the correlation between consecutive states. Balancing exploration and exploitation was also key; the AI needed to explore new strategies while exploiting known successful ones. Initially the AI agent is heavily biased to explore new strategies by taking random actions. Eventually through training it starts to rely more on exploiting learned strategies.\n\n\n\nResults\nOur finished AI, while not exactly championship material, embodies the countless hours of coding and tinkering, the myriad challenges, and the occasional existential debate about the nature of intelligence in video games.\n\n\n\n\n\nIn the video, you can see some of the very first attempts of the AI agent to traverse the first level, 1-1. At this point, the AI is making moves almost completely randomly. He sucks, but understandably. He has no learned experiences yet, so his only option is to just press random buttons and see what sticks. His biggest hurdles seem to be the tall pipe jumps, because it takes continuous jump inputs to get past these, which are rare when you are preseing random buttons.\nAfter 100 hours of training, with 50,000 iterations of experience on this one level under his belt, we present below a successful attempt of the AI on the very first level.\n\n\n\n\n\nHe still seems to get stuck on the pipes, but he is learning, and this AI is clearly better than the first one. What was surprising to me was how long the training process actually took! As students we didn’t have access to unlimited resources in computing, and training this AI for 3 days straight was actually a lot for us. If we had more time, ideally, we could have kept training, and could expect this AI to get better and better over time. If we had the resources, I would love to do this.\nIt’s not perfect, but watching our creation tackle those iconic levels with earnest determination is nothing short of satisfying. In a way, it’s a humbling reminder that even the most sophisticated machines still have a lot to learn from the simplicity of a classic game.\n\n\nReflections\nThis project was a formidable challenge—a crucible that tested our limits and deepened our understanding of reinforcement learning. Our inexperience with coding such algorithms meant grappling with a steep learning curve, compounded by the computational demands of training on limited hardware. Yet, despite the setbacks, we achieved a significant milestone: Mario successfully beat the first level. This outcome, though modest in the grand scheme, stands as a testament to our perseverance and the potency of the Double Deep Q-Network (DDQN) algorithm.\nThrough this endeavor, I honed my skills in coding, neural network design, and the intricacies of reinforcement learning. The project was more than just a technical exercise; it was an immersive dive into the complexities of AI, requiring the synthesis of theory and practice in real time.\nGiven more time and resources, the model’s performance could be significantly enhanced. Extended training on more powerful hardware would allow Mario to refine his decision-making, transforming his erratic movements into deliberate actions. Moreover, exploring advanced algorithms like Proximal Policy Optimization (PPO) and optimizing hyperparameters could unlock new levels of efficiency and performance.\nIn the end, this project was not just about creating an AI that plays a game; it was about pushing the boundaries of what we know and can do with machine learning. The journey was as rewarding as the destination, offering insights that will inform my future work in AI and beyond. The link to the github for this project is here."
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Mental Gymnastics\n\n\n\n\n\n\nsentiment analysis\n\n\nlinguistics\n\n\ntopic modelling\n\n\n\nAnalysis of public stigma in response to athletes’ mental health through tweets about Simone Biles during the 2020 Olympics.\n\n\n\n\n\nMar 22, 2024\n\n\nElena Bateman, Priyanka Iragavarapu, Rebekah Limb, Alyssa Lung, Misha Reiss, Ethan Warren\n\n\n\n\n\n\n\n\n\n\n\n\nSuper Mario Reinforcement Learning\n\n\n\n\n\n\nmachine learning\n\n\nneural network\n\n\ndeep reinforcement learning\n\n\n\nIn a world where Mario doesn’t need your thumbs to save the princess, a neural network takes up the mantle of heroism…\n\n\n\n\n\nMar 13, 2024\n\n\nEthan Warren\n\n\n\n\n\n\n\n\n\n\n\n\nPenguins Classification\n\n\n\n\n\n\nmachine learning\n\n\nscience\n\n\nunsupervised learning\n\n\n\nTraining a classical machine learning model to predict a penguin’s species from physical characteristics.\n\n\n\n\n\nJun 2, 2022\n\n\nChen Guan, Lucas MacHardy, Ethan Warren\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "About Me!",
    "section": "",
    "text": "Here is some text about me."
  }
]